# LLM API Configuration
[llm]
# Default model to use
model = "deepseek/deepseek-r1:free"
base_url = "https://openrouter.ai/api/v1"
api_key = ""
max_tokens = 4000
temperature = 0.7
api_type = "openai"  # Using OpenAI-compatible API format
api_version = ""  # 对于Azure的情况

# Default provider configuration
DEFAULT_PROVIDER = "openai"
DEFAULT_MODEL = "deepseek/deepseek-r1:free"
DEFAULT_TEMPERATURE = 0.7
DEFAULT_MAX_TOKENS = 4096

# LLM provider settings
[LLM_PROVIDERS.openai]
api_type = "openai"
base_url = "https://openrouter.ai/api/v1"
api_key = "sk-or-v1-5953"

# 本地模型配置
[llm.local]
model = "deepseek/deepseek-r1:free"
base_url = "https://openrouter.ai/api/v1"
api_key = ""
max_tokens = 4000
temperature = 0.7
api_type = "openai"
api_version = "" 